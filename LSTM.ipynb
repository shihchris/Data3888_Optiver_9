{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '3888'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(3888)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.random.set_seed(3888)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_seconds(g):\n",
    "    tid = g['time_id'].iloc[0]\n",
    "    full = pd.DataFrame({'seconds_in_bucket': range(600)})\n",
    "    full['time_id'] = tid\n",
    "    return full.merge(g, on=['time_id', 'seconds_in_bucket'], how='left').sort_values('seconds_in_bucket').ffill().bfill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realized_vol(df_slice):\n",
    "    mid = (df_slice['bid_price1'] + df_slice['ask_price1']) / 2\n",
    "    lr = np.log(mid).diff().dropna()\n",
    "    return lr.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df, win=60):\n",
    "    seq_list, stat_list, y_list = [], [], []\n",
    "    for tid, grp in df.groupby('time_id'):\n",
    "        grp = grp.sort_values('seconds_in_bucket')\n",
    "        if len(grp) < 600: continue\n",
    "        pre = grp[grp['seconds_in_bucket'] < 540]\n",
    "        post = grp[(grp['seconds_in_bucket'] >= 540) & (grp['seconds_in_bucket'] < 600)]\n",
    "        windows = []\n",
    "        for i in range(0, 540, win):\n",
    "            w = pre.iloc[i:i+win]\n",
    "            vol = realized_vol(w)\n",
    "            spr = (w['ask_price1'] - w['bid_price1']).mean()\n",
    "            imb = ((w['bid_size1'] - w['ask_size1']) / (w['bid_size1'] + w['ask_size1'] + 1e-9)).mean()\n",
    "            dep = w[['bid_size1','ask_size1','bid_size2','ask_size2']].sum(axis=1).mean()\n",
    "            windows.append([vol, spr, imb, dep])\n",
    "        seq_arr = np.array(windows)\n",
    "        vols = seq_arr[:, 0]\n",
    "        trend = np.diff(vols)\n",
    "        trend2 = np.diff(trend)\n",
    "        mid_series = (pre['bid_price1'] + pre['ask_price1']) / 2\n",
    "        spread_series = (pre['ask_price1'] - pre['bid_price1'])\n",
    "        depth_series = pre[['bid_size1','ask_size1','bid_size2','ask_size2']].sum(axis=1)\n",
    "        static = np.concatenate([\n",
    "            trend, trend2,\n",
    "            [mid_series.mean(), mid_series.std()],\n",
    "            [spread_series.mean(), spread_series.std()],\n",
    "            [depth_series.mean(), depth_series.sum()],\n",
    "            [np.log(mid_series.iloc[-1]+1e-9) - np.log(mid_series.iloc[-2]+1e-9)]\n",
    "        ])\n",
    "        seq_list.append(seq_arr)\n",
    "        stat_list.append(static)\n",
    "        y_list.append(realized_vol(post))\n",
    "    return np.stack(seq_list), np.stack(stat_list), np.array(y_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(seq_len, d_model, static_dim):\n",
    "    seq_in = Input(shape=(seq_len, d_model), name='seq_input')\n",
    "    x = LSTM(64)(seq_in)\n",
    "    stat_in = Input(shape=(static_dim,), name='static_input')\n",
    "    merged = Concatenate()([x, stat_in])\n",
    "    out = Dense(32, activation='relu')(merged)\n",
    "    out = Dense(1, activation='linear')(out)\n",
    "    return Model([seq_in, stat_in], out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    stock_id = 'stock_20'\n",
    "    data_dir = 'individual_book_train'\n",
    "    test_size = 0.10\n",
    "    epochs = 150\n",
    "    batch_size = 32\n",
    "\n",
    "    df = pd.read_csv(os.path.join(data_dir, f'{stock_id}.csv'))\n",
    "    df = df.groupby('time_id', group_keys=False).apply(fill_missing_seconds)\n",
    "\n",
    "    seq, stat, y_raw = make_features(df)\n",
    "    y = np.log(y_raw + 1e-6) * 1e4\n",
    "    y_scaler = StandardScaler()\n",
    "    y = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    N, L, D = seq.shape\n",
    "    seq = StandardScaler().fit_transform(seq.reshape(-1, D)).reshape(N, L, D)\n",
    "    stat = StandardScaler().fit_transform(stat)\n",
    "\n",
    "    seq_tr, seq_te, stat_tr, stat_te, y_tr, y_te = train_test_split(\n",
    "        seq, stat, y, test_size=test_size, random_state=3888\n",
    "    )\n",
    "\n",
    "    model = build_lstm(seq_len=L, d_model=D, static_dim=stat.shape[1])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6)\n",
    "\n",
    "    model.fit(\n",
    "        {'seq_input': seq_tr, 'static_input': stat_tr}, y_tr,\n",
    "        validation_data=({'seq_input': seq_te, 'static_input': stat_te}, y_te),\n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        callbacks=[es, rlr], verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred_std = model.predict({'seq_input': seq_te, 'static_input': stat_te}, verbose=0).flatten()\n",
    "    y_pred_raw = np.exp(y_scaler.inverse_transform(y_pred_std.reshape(-1,1)).flatten() / 1e4) - 1e-6\n",
    "    y_te_raw = np.exp(y_scaler.inverse_transform(y_te.reshape(-1,1)).flatten() / 1e4) - 1e-6\n",
    "\n",
    "    r2 = r2_score(y_te_raw, y_pred_raw)\n",
    "    mae_raw = np.mean(np.abs(y_pred_raw - y_te_raw))\n",
    "\n",
    "    print(f\"Test MAE: {mae_raw:.6f}\")\n",
    "    print(f\"Test R²:  {r2:.5f}\")\n",
    "\n",
    "    for t, p in zip(y_te_raw[:5], y_pred_raw[:5]):\n",
    "        print(f'True: {t:.6f} | Pred: {p:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/2_7k9bc131j9sj_dv4w7_prr0000gn/T/ipykernel_1715/3141181199.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('time_id', group_keys=False).apply(fill_missing_seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.000019\n",
      "Test R²:  0.84352\n",
      "True: 0.000041 | Pred: 0.000041\n",
      "True: 0.000017 | Pred: 0.000030\n",
      "True: 0.000046 | Pred: 0.000043\n",
      "True: 0.000071 | Pred: 0.000059\n",
      "True: 0.000067 | Pred: 0.000071\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
